[
    {
        "title": "Are Classes Clusters?",
        "summary": "  Sentence embedding models aim to provide general purpose embeddings for\nsentences. Most of the models studied in this paper claim to perform well on\nSTS tasks - but they do not report on their suitability for clustering. This\npaper looks at four recent sentence embedding models (Universal Sentence\nEncoder (Cer et al., 2018), Sentence-BERT (Reimers and Gurevych, 2019), LASER\n(Artetxe and Schwenk, 2019), and DeCLUTR (Giorgi et al., 2020)). It gives a\nbrief overview of the ideas behind their implementations. It then investigates\nhow well topic classes in two text classification datasets (Amazon Reviews (Ni\net al., 2019) and News Category Dataset (Misra, 2018)) map to clusters in their\ncorresponding sentence embedding space. While the performance of the resulting\nclassification model is far from perfect, it is better than random. This is\ninteresting because the classification model has been constructed in an\nunsupervised way. The topic classes in these real life topic classification\ndatasets can be partly reconstructed by clustering the corresponding sentence\nembeddings.\n",
        "id": 0
    },
    {
        "title": "Multivalent Entailment Graphs for Question Answering",
        "summary": "  Drawing inferences between open-domain natural language predicates is a\nnecessity for true language understanding. There has been much progress in\nunsupervised learning of entailment graphs for this purpose. We make three\ncontributions: (1) we reinterpret the Distributional Inclusion Hypothesis to\nmodel entailment between predicates of different valencies, like DEFEAT(Biden,\nTrump) entails WIN(Biden); (2) we actualize this theory by learning\nunsupervised Multivalent Entailment Graphs of open-domain predicates; and (3)\nwe demonstrate the capabilities of these graphs on a novel question answering\ntask. We show that directional entailment is more helpful for inference than\nbidirectional similarity on questions of fine-grained semantics. We also show\nthat drawing on evidence across valencies answers more questions than by using\nonly the same valency evidence.\n",
        "id": 1
    },
    {
        "title": "Comparison of Grammatical Error Correction Using Back-Translation Models",
        "summary": "  Grammatical error correction (GEC) suffers from a lack of sufficient parallel\ndata. Therefore, GEC studies have developed various methods to generate pseudo\ndata, which comprise pairs of grammatical and artificially produced\nungrammatical sentences. Currently, a mainstream approach to generate pseudo\ndata is back-translation (BT). Most previous GEC studies using BT have employed\nthe same architecture for both GEC and BT models. However, GEC models have\ndifferent correction tendencies depending on their architectures. Thus, in this\nstudy, we compare the correction tendencies of the GEC models trained on pseudo\ndata generated by different BT models, namely, Transformer, CNN, and LSTM. The\nresults confirm that the correction tendencies for each error type are\ndifferent for every BT model. Additionally, we examine the correction\ntendencies when using a combination of pseudo data generated by different BT\nmodels. As a result, we find that the combination of different BT models\nimproves or interpolates the F_0.5 scores of each error type compared with that\nof single BT models with different seeds.\n",
        "id": 2
    },
    {
        "title": "Segmenting Subtitles for Correcting ASR Segmentation Errors",
        "summary": "  Typical ASR systems segment the input audio into utterances using purely\nacoustic information, which may not resemble the sentence-like units that are\nexpected by conventional machine translation (MT) systems for Spoken Language\nTranslation. In this work, we propose a model for correcting the acoustic\nsegmentation of ASR models for low-resource languages to improve performance on\ndownstream tasks. We propose the use of subtitles as a proxy dataset for\ncorrecting ASR acoustic segmentation, creating synthetic acoustic utterances by\nmodeling common error modes. We train a neural tagging model for correcting ASR\nacoustic segmentation and show that it improves downstream performance on MT\nand audio-document cross-language information retrieval (CLIR).\n",
        "id": 3
    },
    {
        "title": "Probing Across Time: What Does RoBERTa Know and When?",
        "summary": "  Models of language trained on very large corpora have been demonstrated\nuseful for NLP. As fixed artifacts, they have become the object of intense\nstudy, with many researchers \"probing\" the extent to which linguistic\nabstractions, factual and commonsense knowledge, and reasoning abilities they\nacquire and readily demonstrate. Building on this line of work, we consider a\nnew question: for types of knowledge a language model learns, when during\n(pre)training are they acquired? We plot probing performance across iterations,\nusing RoBERTa as a case study. Among our findings: linguistic knowledge is\nacquired fast, stably, and robustly across domains. Facts and commonsense are\nslower and more domain-sensitive. Reasoning abilities are, in general, not\nstably acquired. As new datasets, pretraining protocols, and probes emerge, we\nbelieve that probing-across-time analyses can help researchers understand the\ncomplex, intermingled learning that these models undergo and guide us toward\nmore efficient approaches that accomplish necessary learning faster.\n",
        "id": 4
    },
    {
        "title": "A Comparative Study on Collecting High-Quality Implicit Reasonings at a\n  Large-scale",
        "summary": "  Explicating implicit reasoning (i.e. warrants) in arguments is a\nlong-standing challenge for natural language understanding systems. While\nrecent approaches have focused on explicating warrants via crowdsourcing or\nexpert annotations, the quality of warrants has been questionable due to the\nextreme complexity and subjectivity of the task. In this paper, we tackle the\ncomplex task of warrant explication and devise various methodologies for\ncollecting warrants. We conduct an extensive study with trained experts to\nevaluate the resulting warrants of each methodology and find that our\nmethodologies allow for high-quality warrants to be collected. We construct a\npreliminary dataset of 6,000 warrants annotated over 600 arguments for 3\ndebatable topics. To facilitate research in related downstream tasks, we\nrelease our guidelines and preliminary dataset.\n",
        "id": 5
    },
    {
        "title": "Optimal Size-Performance Tradeoffs: Weighing PoS Tagger Models",
        "summary": "  Improvement in machine learning-based NLP performance are often presented\nwith bigger models and more complex code. This presents a trade-off: better\nscores come at the cost of larger tools; bigger models tend to require more\nduring training and inference time. We present multiple methods for measuring\nthe size of a model, and for comparing this with the model's performance.\n  In a case study over part-of-speech tagging, we then apply these techniques\nto taggers for eight languages and present a novel analysis identifying which\ntaggers are size-performance optimal. Results indicate that some classical\ntaggers place on the size-performance skyline across languages. Further,\nalthough the deep models have highest performance for multiple scores, it is\noften not the most complex of these that reach peak performance.\n",
        "id": 6
    },
    {
        "title": "ProphetNet-X: Large-Scale Pre-training Models for English, Chinese,\n  Multi-lingual, Dialog, and Code Generation",
        "summary": "  Now, the pre-training technique is ubiquitous in natural language processing\nfield. ProphetNet is a pre-training based natural language generation method\nwhich shows powerful performance on English text summarization and question\ngeneration tasks. In this paper, we extend ProphetNet into other domains and\nlanguages, and present the ProphetNet family pre-training models, named\nProphetNet-X, where X can be English, Chinese, Multi-lingual, and so on. We\npre-train a cross-lingual generation model ProphetNet-Multi, a Chinese\ngeneration model ProphetNet-Zh, two open-domain dialog generation models\nProphetNet-Dialog-En and ProphetNet-Dialog-Zh. And also, we provide a PLG\n(Programming Language Generation) model ProphetNet-Code to show the generation\nperformance besides NLG (Natural Language Generation) tasks. In our\nexperiments, ProphetNet-X models achieve new state-of-the-art performance on 10\nbenchmarks. All the models of ProphetNet-X share the same model structure,\nwhich allows users to easily switch between different models. We make the code\nand models publicly available, and we will keep updating more pre-training\nmodels and finetuning scripts.\n",
        "id": 7
    },
    {
        "title": "Cost-effective End-to-end Information Extraction for Semi-structured\n  Document Images",
        "summary": "  A real-world information extraction (IE) system for semi-structured document\nimages often involves a long pipeline of multiple modules, whose complexity\ndramatically increases its development and maintenance cost. One can instead\nconsider an end-to-end model that directly maps the input to the target output\nand simplify the entire process. However, such generation approach is known to\nlead to unstable performance if not designed carefully. Here we present our\nrecent effort on transitioning from our existing pipeline-based IE system to an\nend-to-end system focusing on practical challenges that are associated with\nreplacing and deploying the system in real, large-scale production. By\ncarefully formulating document IE as a sequence generation task, we show that a\nsingle end-to-end IE system can be built and still achieve competent\nperformance.\n",
        "id": 8
    },
    {
        "title": "Improving Zero-Shot Multi-Lingual Entity Linking",
        "summary": "  Entity linking -- the task of identifying references in free text to relevant\nknowledge base representations -- often focuses on single languages. We\nconsider multilingual entity linking, where a single model is trained to link\nreferences to same-language knowledge bases in several languages. We propose a\nneural ranker architecture, which leverages multilingual transformer\nrepresentations of text to be easily applied to a multilingual setting. We then\nexplore how a neural ranker trained in one language (e.g. English) transfers to\nan unseen language (e.g. Chinese), and find that while there is a consistent\nbut not large drop in performance. How can this drop in performance be\nalleviated? We explore adding an adversarial objective to force our model to\nlearn language-invariant representations. We find that using this approach\nimproves recall in several datasets, often matching the in-language\nperformance, thus alleviating some of the performance loss occurring from\nzero-shot transfer.\n",
        "id": 9
    },
    {
        "title": "LU-BZU at SemEval-2021 Task 2: Word2Vec and Lemma2Vec performance in\n  Arabic Word-in-Context disambiguation",
        "summary": "  This paper presents a set of experiments to evaluate and compare between the\nperformance of using CBOW Word2Vec and Lemma2Vec models for Arabic\nWord-in-Context (WiC) disambiguation without using sense inventories or sense\nembeddings. As part of the SemEval-2021 Shared Task 2 on WiC disambiguation, we\nused the dev.ar-ar dataset (2k sentence pairs) to decide whether two words in a\ngiven sentence pair carry the same meaning. We used two Word2Vec models:\nWiki-CBOW, a pre-trained model on Arabic Wikipedia, and another model we\ntrained on large Arabic corpora of about 3 billion tokens. Two Lemma2Vec models\nwas also constructed based on the two Word2Vec models. Each of the four models\nwas then used in the WiC disambiguation task, and then evaluated on the\nSemEval-2021 test.ar-ar dataset. At the end, we reported the performance of\ndifferent models and compared between using lemma-based and word-based models.\n",
        "id": 10
    },
    {
        "title": "Temporal Adaptation of BERT and Performance on Downstream Document\n  Classification: Insights from Social Media",
        "summary": "  Language use differs between domains and even within a domain, language use\nchanges over time. For pre-trained language models like BERT, domain adaptation\nthrough continued pre-training has been shown to improve performance on\nin-domain downstream tasks. In this article, we investigate whether temporal\nadaptation can bring additional benefits. For this purpose, we introduce a\ncorpus of social media comments sampled over three years. It contains\nunlabelled data for adaptation and evaluation on an upstream masked language\nmodelling task as well as labelled data for fine-tuning and evaluation on a\ndownstream document classification task. We find that temporality matters for\nboth tasks: temporal adaptation improves upstream and temporal fine-tuning\ndownstream task performance. Time-specific models generally perform better on\npast than on future test sets, which matches evidence on the bursty usage of\ntopical words. However, adapting BERT to time and domain does not improve\nperformance on the downstream task over only adapting to domain. Token-level\nanalysis shows that temporal adaptation captures event-driven changes in\nlanguage use in the downstream task, but not those changes that are actually\nrelevant to task performance. Based on our findings, we discuss when temporal\nadaptation may be more effective.\n",
        "id": 11
    },
    {
        "title": "Towards Variable-Length Textual Adversarial Attacks",
        "summary": "  Adversarial attacks have shown the vulnerability of machine learning models,\nhowever, it is non-trivial to conduct textual adversarial attacks on natural\nlanguage processing tasks due to the discreteness of data. Most previous\napproaches conduct attacks with the atomic \\textit{replacement} operation,\nwhich usually leads to fixed-length adversarial examples and therefore limits\nthe exploration on the decision space. In this paper, we propose\nvariable-length textual adversarial attacks~(VL-Attack) and integrate three\natomic operations, namely \\textit{insertion}, \\textit{deletion} and\n\\textit{replacement}, into a unified framework, by introducing and manipulating\na special \\textit{blank} token while attacking. In this way, our approach is\nable to more comprehensively find adversarial examples around the decision\nboundary and effectively conduct adversarial attacks. Specifically, our method\ndrops the accuracy of IMDB classification by $96\\%$ with only editing $1.3\\%$\ntokens while attacking a pre-trained BERT model. In addition, fine-tuning the\nvictim model with generated adversarial samples can improve the robustness of\nthe model without hurting the performance, especially for length-sensitive\nmodels. On the task of non-autoregressive machine translation, our method can\nachieve $33.18$ BLEU score on IWSLT14 German-English translation, achieving an\nimprovement of $1.47$ over the baseline model.\n",
        "id": 12
    },
    {
        "title": "Back to Square One: Artifact Detection, Training and Commonsense\n  Disentanglement in the Winograd Schema",
        "summary": "  The Winograd Schema (WS) has been proposed as a test for measuring\ncommonsense capabilities of models. Recently, pre-trained language model-based\napproaches have boosted performance on some WS benchmarks but the source of\nimprovement is still not clear. This paper suggests that the apparent progress\non WS may not necessarily reflect progress in commonsense reasoning. To support\nthis claim, we first show that the current evaluation method of WS is\nsub-optimal and propose a modification that uses twin sentences for evaluation.\nWe also propose two new baselines that indicate the existence of artifacts in\nWS benchmarks. We then develop a method for evaluating WS-like sentences in a\nzero-shot setting to account for the commonsense reasoning abilities acquired\nduring the pretraining and observe that popular language models perform\nrandomly in this setting when using our more strict evaluation. We conclude\nthat the observed progress is mostly due to the use of supervision in training\nWS models, which is not likely to successfully support all the required\ncommonsense reasoning skills and knowledge.\n",
        "id": 13
    },
    {
        "title": "Word2rate: training and evaluating multiple word embeddings as\n  statistical transitions",
        "summary": "  Using pretrained word embeddings has been shown to be a very effective way in\nimproving the performance of natural language processing tasks. In fact almost\nany natural language tasks that can be thought of has been improved by these\npretrained embeddings. These tasks range from sentiment analysis, translation,\nsequence prediction amongst many others. One of the most successful word\nembeddings is the Word2vec CBOW model proposed by Mikolov trained by the\nnegative sampling technique. Mai et al. modifies this objective to train CMOW\nembeddings that are sensitive to word order. We used a modified version of the\nnegative sampling objective for our context words, modelling the context\nembeddings as a Taylor series of rate matrices. We show that different modes of\nthe Taylor series produce different types of embeddings. We compare these\nembeddings to their similar counterparts like CBOW and CMOW and show that they\nachieve comparable performance. We also introduce a novel left-right context\nsplit objective that improves performance for tasks sensitive to word order.\nOur Word2rate model is grounded in a statistical foundation using rate matrices\nwhile being competitive in variety of language tasks.\n",
        "id": 14
    },
    {
        "title": "IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural\n  Language Generation",
        "summary": "  Natural language generation (NLG) benchmarks provide an important avenue to\nmeasure progress and develop better NLG systems. Unfortunately, the lack of\npublicly available NLG benchmarks for low-resource languages poses a\nchallenging barrier for building NLG systems that work well for languages with\nlimited amounts of data. Here we introduce IndoNLG, the first benchmark to\nmeasure natural language generation (NLG) progress in three low-resource -- yet\nwidely spoken -- languages of Indonesia: Indonesian, Javanese, and Sundanese.\nAltogether, these languages are spoken by more than 100 million native\nspeakers, and hence constitute an important use case of NLG systems today.\nConcretely, IndoNLG covers six tasks: summarization, question answering,\nchit-chat, and three different pairs of machine translation (MT) tasks. We\ncollate a clean pretraining corpus of Indonesian, Sundanese, and Javanese\ndatasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and\nIndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on\nall tasks -- despite using only one-fifth the parameters of a larger\nmultilingual model, mBART-LARGE (Liu et al., 2020). This finding emphasizes the\nimportance of pretraining on closely related, local languages to achieve more\nefficient learning and faster inference for very low-resource languages like\nJavanese and Sundanese.\n",
        "id": 15
    },
    {
        "title": "$Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues\n  via Question Generation and Question Answering",
        "summary": "  Neural knowledge-grounded generative models for dialogue often produce\ncontent that is factually inconsistent with the knowledge they rely on, making\nthem unreliable and limiting their applicability. Inspired by recent work on\nevaluating factual consistency in abstractive summarization, we propose an\nautomatic evaluation metric for factual consistency in knowledge-grounded\ndialogue using automatic question generation and question answering. Our\nmetric, denoted $Q^2$, compares answer spans using natural language inference\n(NLI), instead of token-based matching as done in previous work. To foster\nproper evaluation, we curate a novel dataset of dialogue system outputs for the\nWizard-of-Wikipedia dataset, manually annotated for factual consistency. We\nperform a thorough meta-evaluation of $Q^2$ against other metrics using this\ndataset and two others, where it consistently shows higher correlation with\nhuman judgements.\n",
        "id": 16
    },
    {
        "title": "Robust Open-Vocabulary Translation from Visual Text Representations",
        "summary": "  Machine translation models have discrete vocabularies and commonly use\nsubword segmentation techniques to achieve an 'open vocabulary.' This approach\nrelies on consistent and correct underlying unicode sequences, and makes models\nsusceptible to degradation from common types of noise and variation. Motivated\nby the robustness of human language processing, we propose the use of visual\ntext representations, which dispense with a finite set of text embeddings in\nfavor of continuous vocabularies created by processing visually rendered text\nwith sliding windows. We show that models using visual text representations\napproach or match performance of traditional text models on small and larger\ndatasets. More importantly, models with visual embeddings demonstrate\nsignificant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a\ncharacter permuted German-English task where subword models degrade to 1.9.\n",
        "id": 17
    },
    {
        "title": "Distantly Supervised Relation Extraction with Sentence Reconstruction\n  and Knowledge Base Priors",
        "summary": "  We propose a multi-task, probabilistic approach to facilitate distantly\nsupervised relation extraction by bringing closer the representations of\nsentences that contain the same Knowledge Base pairs. To achieve this, we bias\nthe latent space of sentences via a Variational Autoencoder (VAE) that is\ntrained jointly with a relation classifier. The latent code guides the pair\nrepresentations and influences sentence reconstruction. Experimental results on\ntwo datasets created via distant supervision indicate that multi-task learning\nresults in performance benefits. Additional exploration of employing Knowledge\nBase priors into the VAE reveals that the sentence space can be shifted towards\nthat of the Knowledge Base, offering interpretability and further improving\nresults.\n",
        "id": 18
    },
    {
        "title": "An Adversarially-Learned Turing Test for Dialog Generation Models",
        "summary": "  The design of better automated dialogue evaluation metrics offers the\npotential of accelerate evaluation research on conversational AI. However,\nexisting trainable dialogue evaluation models are generally restricted to\nclassifiers trained in a purely supervised manner, which suffer a significant\nrisk from adversarial attacking (e.g., a nonsensical response that enjoys a\nhigh classification score). To alleviate this risk, we propose an adversarial\ntraining approach to learn a robust model, ATT (Adversarial Turing Test), that\ndiscriminates machine-generated responses from human-written replies. In\ncontrast to previous perturbation-based methods, our discriminator is trained\nby iteratively generating unrestricted and diverse adversarial examples using\nreinforcement learning. The key benefit of this unrestricted adversarial\ntraining approach is allowing the discriminator to improve robustness in an\niterative attack-defense game. Our discriminator shows high accuracy on strong\nattackers including DialoGPT and GPT-3.\n",
        "id": 19
    },
    {
        "title": "What to Pre-Train on? Efficient Intermediate Task Selection",
        "summary": "  Intermediate task fine-tuning has been shown to culminate in large transfer\ngains across many NLP tasks. With an abundance of candidate datasets as well as\npre-trained language models, it has become infeasible to run the cross-product\nof all combinations to find the best transfer setting. In this work we first\nestablish that similar sequential fine-tuning gains can be achieved in adapter\nsettings, and subsequently consolidate previously proposed methods that\nefficiently identify beneficial tasks for intermediate transfer learning. We\nexperiment with a diverse set of 42 intermediate and 11 target English\nclassification, multiple choice, question answering, and sequence tagging\ntasks. Our results show that efficient embedding based methods that rely solely\non the respective datasets outperform computational expensive few-shot\nfine-tuning approaches. Our best methods achieve an average Regret@3 of less\nthan 1% across all target tasks, demonstrating that we are able to efficiently\nidentify the best datasets for intermediate training.\n",
        "id": 20
    },
    {
        "title": "proScript: Partially Ordered Scripts Generation via Pre-trained Language\n  Models",
        "summary": "  Scripts - standardized event sequences describing typical everyday activities\n- have been shown to help understand narratives by providing expectations,\nresolving ambiguity, and filling in unstated information. However, to date they\nhave proved hard to author or extract from text. In this work, we demonstrate\nfor the first time that pre-trained neural language models (LMs) can be be\nfinetuned to generate high-quality scripts, at varying levels of granularity,\nfor a wide range of everyday scenarios (e.g., bake a cake). To do this, we\ncollected a large (6.4k), crowdsourced partially ordered scripts (named\nproScript), which is substantially larger than prior datasets, and developed\nmodels that generate scripts with combining language generation and structure\nprediction. We define two complementary tasks: (i) edge prediction: given a\nscenario and unordered events, organize the events into a valid (possibly\npartial-order) script, and (ii) script generation: given only a scenario,\ngenerate events and organize them into a (possibly partial-order) script. Our\nexperiments show that our models perform well (e.g., F1=75.7 in task (i)),\nillustrating a new approach to overcoming previous barriers to script\ncollection. We also show that there is still significant room for improvement\ntoward human level performance. Together, our tasks, dataset, and models offer\na new research direction for learning script knowledge.\n",
        "id": 21
    },
    {
        "title": "Context-Adaptive Document-Level Neural Machine Translation",
        "summary": "  Most existing document-level neural machine translation (NMT) models leverage\na fixed number of the previous or all global source sentences to handle the\ncontext-independent problem in standard NMT. However, the translating of each\nsource sentence benefits from various sizes of context, and inappropriate\ncontext may harm the translation performance. In this work, we introduce a\ndata-adaptive method that enables the model to adopt the necessary and useful\ncontext. Specifically, we introduce a light predictor into two document-level\ntranslation models to select the explicit context. Experiments demonstrate the\nproposed approach can significantly improve the performance over the previous\nmethods with a gain up to 1.99 BLEU points.\n",
        "id": 22
    },
    {
        "title": "Learning to Reason for Text Generation from Scientific Tables",
        "summary": "  In this paper, we introduce SciGen, a new challenge dataset for the task of\nreasoning-aware data-to-text generation consisting of tables from scientific\narticles and their corresponding descriptions. Describing scientific tables\ngoes beyond the surface realization of the table content and requires reasoning\nover table values. The unique properties of SciGen are that (1) tables mostly\ncontain numerical values, and (2) the corresponding descriptions require\narithmetic reasoning. SciGen is therefore the first dataset that assesses the\narithmetic reasoning capabilities of generation models on complex input\nstructures, i.e., tables from scientific articles. We study the effectiveness\nof state-of-the-art data-to-text generation models on SciGen and evaluate the\nresults using common metrics as well as human evaluation. Our results and\nanalyses show that (a) while humans like to reason for describing scientific\ntables, the ability of state-of-the-art models is severely limited on this\ntask, (b) while adding more training data improves the results, it is not the\nsolution for reasoning-aware text generation, and (c) one of the main\nbottlenecks for this task is the lack of proper automatic evaluation metrics.\nThe data, code, and annotations for human evaluation will be available at\nhttps://github.com/UKPLab/SciGen. SciGen opens new avenues for future research\nin reasoning-aware text generation and evaluation.\n",
        "id": 23
    },
    {
        "title": "Membership Inference Attack Susceptibility of Clinical Language Models",
        "summary": "  Deep Neural Network (DNN) models have been shown to have high empirical\nprivacy leakages. Clinical language models (CLMs) trained on clinical data have\nbeen used to improve performance in biomedical natural language processing\ntasks. In this work, we investigate the risks of training-data leakage through\nwhite-box or black-box access to CLMs. We design and employ membership\ninference attacks to estimate the empirical privacy leaks for model\narchitectures like BERT and GPT2. We show that membership inference attacks on\nCLMs lead to non-trivial privacy leakages of up to 7%. Our results show that\nsmaller models have lower empirical privacy leakages than larger ones, and\nmasked LMs have lower leakages than auto-regressive LMs. We further show that\ndifferentially private CLMs can have improved model utility on clinical domain\nwhile ensuring low empirical privacy leakage. Lastly, we also study the effects\nof group-level membership inference and disease rarity on CLM privacy leakages.\n",
        "id": 24
    },
    {
        "title": "Surface Form Competition: Why the Highest Probability Answer Isn't\n  Always Right",
        "summary": "  Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n  However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n  We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n",
        "id": 25
    },
    {
        "title": "On the Importance of Effectively Adapting Pretrained Language Models for\n  Active Learning",
        "summary": "  Recent Active Learning (AL) approaches in Natural Language Processing (NLP)\nproposed using off-the-shelf pretrained language models (LMs). In this paper,\nwe argue that these LMs are not adapted effectively to the downstream task\nduring AL and we explore ways to address this issue. We suggest to first adapt\nthe pretrained LM to the target task by continuing training with all the\navailable unlabeled data and then use it for AL. We also propose a simple yet\neffective fine-tuning method to ensure that the adapted LM is properly trained\nin both low and high resource scenarios during AL. Our experiments demonstrate\nthat our approach provides substantial data efficiency improvements compared to\nthe standard fine-tuning approach, suggesting that a poor training strategy can\nbe catastrophic for AL.\n",
        "id": 26
    },
    {
        "title": "ESTER: A Machine Reading Comprehension Dataset for Event Semantic\n  Relation Reasoning",
        "summary": "  Understanding how events are semantically related to each other is the\nessence of reading comprehension. Recent event-centric reading comprehension\ndatasets focus mostly on event arguments or temporal relations. While these\ntasks partially evaluate machines' ability of narrative understanding,\nhuman-like reading comprehension requires the capability to process event-based\ninformation beyond arguments and temporal reasoning. For example, to understand\ncausality between events, we need to infer motivation or purpose; to establish\nevent hierarchy, we need to understand the composition of events. To facilitate\nthese tasks, we introduce ESTER, a comprehensive machine reading comprehension\n(MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages\nnatural language queries to reason about the five most common event semantic\nrelations, provides more than 6K questions and captures 10.1K event relation\npairs. Experimental results show that the current SOTA systems achieve 22.1%,\n63.3%, and 83.5% for token-based exact-match, F1, and event-based HIT@1 scores,\nwhich are all significantly below human performances (36.0%, 79.6%, 100%\nrespectively), highlighting our dataset as a challenging benchmark.\n",
        "id": 27
    },
    {
        "title": "Concadia: Towards Image-Based Text Generation with a Purpose",
        "summary": "  Current deep learning models often achieve excellent results on benchmark\nimage-to-text datasets but fail to generate texts that are useful in practice.\nWe argue that to close this gap, it is vital to distinguish descriptions from\ncaptions based on their distinct communicative roles. Descriptions focus on\nvisual features and are meant to replace an image (often to increase\naccessibility), whereas captions appear alongside an image to supply additional\ninformation. To motivate this distinction and help people put it into practice,\nwe introduce the publicly available Wikipedia-based dataset Concadia consisting\nof 96,918 images with corresponding English-language descriptions, captions,\nand surrounding context. Using insights from Concadia, models trained on it,\nand a preregistered human-subjects experiment with human- and model-generated\ntexts, we characterize the commonalities and differences between descriptions\nand captions. In addition, we show that, for generating both descriptions and\ncaptions, it is useful to augment image-to-text models with representations of\nthe textual context in which the image appeared.\n",
        "id": 28
    },
    {
        "title": "Unsupervised Extractive Summarization by Human Memory Simulation",
        "summary": "  Summarization systems face the core challenge of identifying and selecting\nimportant information. In this paper, we tackle the problem of content\nselection in unsupervised extractive summarization of long, structured\ndocuments. We introduce a wide range of heuristics that leverage cognitive\nrepresentations of content units and how these are retained or forgotten in\nhuman memory. We find that properties of these representations of human memory\ncan be exploited to capture relevance of content units in scientific articles.\nExperiments show that our proposed heuristics are effective at leveraging\ncognitive structures and the organization of the document (i.e.\\ sections of an\narticle), and automatic and human evaluations provide strong evidence that\nthese heuristics extract more summary-worthy content units.\n",
        "id": 29
    },
    {
        "title": "Re-TACRED: Addressing Shortcomings of the TACRED Dataset",
        "summary": "  TACRED is one of the largest and most widely used sentence-level relation\nextraction datasets. Proposed models that are evaluated using this dataset\nconsistently set new state-of-the-art performance. However, they still exhibit\nlarge error rates despite leveraging external knowledge and unsupervised\npretraining on large text corpora. A recent study suggested that this may be\ndue to poor dataset quality. The study observed that over 50% of the most\nchallenging sentences from the development and test sets are incorrectly\nlabeled and account for an average drop of 8% f1-score in model performance.\nHowever, this study was limited to a small biased sample of 5k (out of a total\nof 106k) sentences, substantially restricting the generalizability and broader\nimplications of its findings. In this paper, we address these shortcomings by:\n(i) performing a comprehensive study over the whole TACRED dataset, (ii)\nproposing an improved crowdsourcing strategy and deploying it to re-annotate\nthe whole dataset, and (iii) performing a thorough analysis to understand how\ncorrecting the TACRED annotations affects previously published results. After\nverification, we observed that 23.9% of TACRED labels are incorrect. Moreover,\nevaluating several models on our revised dataset yields an average f1-score\nimprovement of 14.3% and helps uncover significant relationships between the\ndifferent models (rather than simply offsetting or scaling their scores by a\nconstant factor). Finally, aside from our analysis we also release Re-TACRED, a\nnew completely re-annotated version of the TACRED dataset that can be used to\nperform reliable evaluation of relation extraction models.\n",
        "id": 30
    },
    {
        "title": "Structure-Aware Abstractive Conversation Summarization via Discourse and\n  Action Graphs",
        "summary": "  Abstractive conversation summarization has received much attention recently.\nHowever, these generated summaries often suffer from insufficient, redundant,\nor incorrect content, largely due to the unstructured and complex\ncharacteristics of human-human interactions. To this end, we propose to\nexplicitly model the rich structures in conversations for more precise and\naccurate conversation summarization, by first incorporating discourse relations\nbetween utterances and action triples (\"who-doing-what\") in utterances through\nstructured graphs to better encode conversations, and then designing a\nmulti-granularity decoder to generate summaries by combining all levels of\ninformation. Experiments show that our proposed models outperform\nstate-of-the-art methods and generalize well in other domains in terms of both\nautomatic evaluations and human judgments. We have publicly released our code\nat https://github.com/GT-SALT/Structure-Aware-BART.\n",
        "id": 31
    },
    {
        "title": "Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained\n  Models",
        "summary": "  There is growing evidence that pretrained language models improve\ntask-specific fine-tuning not just for the languages seen in pretraining, but\nalso for new languages and even non-linguistic data. What is the nature of this\nsurprising cross-domain transfer? We offer a partial answer via a systematic\nexploration of how much transfer occurs when models are denied any information\nabout word identity via random scrambling. In four classification tasks and two\nsequence labeling tasks, we evaluate baseline models, LSTMs using GloVe\nembeddings, and BERT. We find that only BERT shows high rates of transfer into\nour scrambled domains, and for classification but not sequence labeling tasks.\nOur analyses seek to explain why transfer succeeds for some tasks but not\nothers, to isolate the separate contributions of pretraining versus\nfine-tuning, and to quantify the role of word frequency. These findings help\nexplain where and why cross-domain transfer occurs, which can guide future\nstudies and practical fine-tuning efforts.\n",
        "id": 32
    },
    {
        "title": "Sequential Cross-Document Coreference Resolution",
        "summary": "  Relating entities and events in text is a key component of natural language\nunderstanding. Cross-document coreference resolution, in particular, is\nimportant for the growing interest in multi-document analysis tasks. In this\nwork we propose a new model that extends the efficient sequential prediction\nparadigm for coreference resolution to cross-document settings and achieves\ncompetitive results for both entity and event coreference while provides strong\nevidence of the efficacy of both sequential models and higher-order inference\nin cross-document settings. Our model incrementally composes mentions into\ncluster representations and predicts links between a mention and the already\nconstructed clusters, approximating a higher-order model. In addition, we\nconduct extensive ablation studies that provide new insights into the\nimportance of various inputs and representation types in coreference.\n",
        "id": 33
    },
    {
        "title": "A Full Text-Dependent End to End Mispronunciation Detection and\n  Diagnosis with Easy Data Augmentation Techniques",
        "summary": "  Recently, end-to-end mispronunciation detection and diagnosis (MD&D) systems\nhas become a popular alternative to greatly simplify the model-building process\nof conventional hybrid DNN-HMM systems by representing complicated modules with\na single deep network architecture. In this paper, in order to utilize the\nprior text in the end-to-end structure, we present a novel text-dependent model\nwhich is difference with sed-mdd, the model achieves a fully end-to-end system\nby aligning the audio with the phoneme sequences of the prior text inside the\nmodel through the attention mechanism. Moreover, the prior text as input will\nbe a problem of imbalance between positive and negative samples in the phoneme\nsequence. To alleviate this problem, we propose three simple data augmentation\nmethods, which effectively improve the ability of model to capture\nmispronounced phonemes. We conduct experiments on L2-ARCTIC, and our best\nperformance improved from 49.29% to 56.08% in F-measure metric compared to the\nCNN-RNN-CTC model.\n",
        "id": 34
    },
    {
        "title": "Three-level Hierarchical Transformer Networks for Long-sequence and\n  Multiple Clinical Documents Classification",
        "summary": "  We present a Three-level Hierarchical Transformer Network (3-level-HTN) for\nmodeling long-term dependencies across clinical notes for the purpose of\npatient-level prediction. The network is equipped with three levels of\nTransformer-based encoders to learn progressively from words to sentences,\nsentences to notes, and finally notes to patients. The first level from word to\nsentence directly applies a pre-trained BERT model as a fully trainable\ncomponent. While the second and third levels both implement a stack of\ntransformer-based encoders, before the final patient representation is fed into\na classification layer for clinical predictions. Compared to conventional BERT\nmodels, our model increases the maximum input length from 512 tokens to much\nlonger sequences that are appropriate for modeling large numbers of clinical\nnotes. We empirically examine different hyper-parameters to identify an optimal\ntrade-off given computational resource limits. Our experiment results on the\nMIMIC-III dataset for different prediction tasks demonstrate that the proposed\nHierarchical Transformer Network outperforms previous state-of-the-art models,\nincluding but not limited to BigBird.\n",
        "id": 35
    },
    {
        "title": "Data Distillation for Text Classification",
        "summary": "  Deep learning techniques have achieved great success in many fields, while at\nthe same time deep learning models are getting more complex and expensive to\ncompute. It severely hinders the wide applications of these models. In order to\nalleviate this problem, model distillation emerges as an effective means to\ncompress a large model into a smaller one without a significant drop in\naccuracy. In this paper, we study a related but orthogonal issue, data\ndistillation, which aims to distill the knowledge from a large training dataset\ndown to a smaller and synthetic one. It has the potential to address the large\nand growing neural network training problem based on the small dataset. We\ndevelop a novel data distillation method for text classification. We evaluate\nour method on eight benchmark datasets. The results that the distilled data\nwith the size of 0.1% of the original text data achieves approximately 90%\nperformance of the original is rather impressive.\n",
        "id": 36
    },
    {
        "title": "R&R: Metric-guided Adversarial Sentence Generation",
        "summary": "  Adversarial examples are helpful for analyzing and improving the robustness\nof text classifiers. Generating high-quality adversarial examples is a\nchallenging task as it requires generating fluent adversarial sentences that\nare semantically similar to the original sentences and preserve the original\nlabels, while causing the classifier to misclassify them. Existing methods\nprioritize misclassification by maximizing each perturbation's effectiveness at\nmisleading a text classifier; thus, the generated adversarial examples fall\nshort in terms of fluency and similarity. In this paper, we propose a rewrite\nand rollback (R&R) framework for adversarial attack. It improves the quality of\nadversarial examples by optimizing a critique score which combines the fluency,\nsimilarity, and misclassification metrics. R&R generates high-quality\nadversarial examples by allowing exploration of perturbations that do not have\nimmediate impact on the misclassification metric but can improve fluency and\nsimilarity metrics. We evaluate our method on 5 representative datasets and 3\nclassifier architectures. Our method outperforms current state-of-the-art in\nattack success rate by +16.2%, +12.8%, and +14.0% on the classifiers\nrespectively. Code is available at https://github.com/DAI-Lab/fibber\n",
        "id": 37
    },
    {
        "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path\n  Grounding",
        "summary": "  Dialogue systems powered by large pre-trained language models (LM) exhibit an\ninnate ability to deliver fluent and natural-looking responses. Despite their\nimpressive generation performance, these models can often generate factually\nincorrect statements impeding their widespread adoption. In this paper, we\nfocus on the task of improving the faithfulness -- and thus reduce\nhallucination -- of Neural Dialogue Systems to known facts supplied by a\nKnowledge Graph (KG). We propose Neural Path Hunter which follows a\ngenerate-then-refine strategy whereby a generated response is amended using the\nk-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level\nfact critic to identify plausible sources of hallucination followed by a\nrefinement stage consisting of a chain of two neural LM's that retrieves\ncorrect entities by crafting a query signal that is propagated over the k-hop\nsubgraph. Our proposed model can easily be applied to any dialogue generated\nresponses without retraining the model. We empirically validate our proposed\napproach on the OpenDialKG dataset against a suite of metrics and report a\nrelative improvement of faithfulness over dialogue responses by 20.35% based on\nFeQA (Durmus et al., 2020).\n",
        "id": 38
    },
    {
        "title": "Moving on from OntoNotes: Coreference Resolution Model Transfer",
        "summary": "  Academic neural models for coreference resolution (coref) are typically\ntrained on a single dataset, OntoNotes, and model improvements are benchmarked\non that same dataset. However, real-world applications of coref depend on the\nannotation guidelines and the domain of the target dataset, which often differ\nfrom those of OntoNotes. We aim to quantify transferability of coref models\nbased on the number of annotated documents available in the target dataset. We\nexamine eleven target datasets and find that continued training is consistently\neffective and especially beneficial when there are few target documents. We\nestablish new benchmarks across several datasets, including state-of-the-art\nresults on PreCo.\n",
        "id": 39
    },
    {
        "title": "Syntactic structures and the general Markov models",
        "summary": "  We study phylogenetic signal present in syntactic information by considering\nthe syntactic structures data from Longobardi (2017b), Collins (2010), Ceolin\net al. (2020) and Koopman (2011). Focusing first on the general Markov models,\nwe explore how well the the syntactic structures data conform to the hypothesis\nrequired by these models. We do this by comparing derived phylogenetic trees\nagainst trees agreed on by the linguistics community. We then interpret the\nmethods of Ceolin et al. (2020) as an infinite sites evolutionary model and\ncompare the consistency of the data with this alternative. The ideas and\nmethods discussed in the present paper are more generally applicable than to\nthe specific setting of syntactic structures, and can be used in other\ncontexts, when analyzing consistency of data with against hypothesized\nevolutionary models.\n",
        "id": 40
    },
    {
        "title": "A multilabel approach to morphosyntactic probing",
        "summary": "  We introduce a multilabel probing task to assess the morphosyntactic\nrepresentations of word embeddings from multilingual language models. We\ndemonstrate this task with multilingual BERT (Devlin et al., 2018), training\nprobes for seven typologically diverse languages of varying morphological\ncomplexity: Afrikaans, Croatian, Finnish, Hebrew, Korean, Spanish, and Turkish.\nThrough this simple but robust paradigm, we show that multilingual BERT renders\nmany morphosyntactic features easily and simultaneously extractable (e.g.,\ngender, grammatical case, pronominal type). We further evaluate the probes on\nsix \"held-out\" languages in a zero-shot transfer setting: Arabic, Chinese,\nMarathi, Slovenian, Tagalog, and Yoruba. This style of probing has the added\nbenefit of revealing the linguistic properties that language models recognize\nas being shared across languages. For instance, the probes performed well on\nrecognizing nouns in the held-out languages, suggesting that multilingual BERT\nhas a conception of noun-hood that transcends individual languages; yet, the\nsame was not true of adjectives.\n",
        "id": 41
    },
    {
        "title": "Frequency-based Distortions in Contextualized Word Embeddings",
        "summary": "  How does word frequency in pre-training data affect the behavior of\nsimilarity metrics in contextualized BERT embeddings? Are there systematic ways\nin which some word relationships are exaggerated or understated? In this work,\nwe explore the geometric characteristics of contextualized word embeddings with\ntwo novel tools: (1) an identity probe that predicts the identity of a word\nusing its embedding; (2) the minimal bounding sphere for a word's\ncontextualized representations. Our results reveal that words of high and low\nfrequency differ significantly with respect to their representational geometry.\nSuch differences introduce distortions: when compared to human judgments, point\nestimates of embedding similarity (e.g., cosine similarity) can over- or\nunder-estimate the semantic similarity of two words, depending on the frequency\nof those words in the training data. This has downstream societal implications:\nBERT-Base has more trouble differentiating between South American and African\ncountries than North American and European ones. We find that these distortions\npersist when using BERT-Multilingual, suggesting that they cannot be easily\nfixed with additional data, which in turn introduces new distortions.\n",
        "id": 42
    },
    {
        "title": "Sentence Concatenation Approach to Data Augmentation for Neural Machine\n  Translation",
        "summary": "  Neural machine translation (NMT) has recently gained widespread attention\nbecause of its high translation accuracy. However, it shows poor performance in\nthe translation of long sentences, which is a major issue in low-resource\nlanguages. It is assumed that this issue is caused by insufficient number of\nlong sentences in the training data. Therefore, this study proposes a simple\ndata augmentation method to handle long sentences. In this method, we use only\nthe given parallel corpora as the training data and generate long sentences by\nconcatenating two sentences. Based on the experimental results, we confirm\nimprovements in long sentence translation by the proposed data augmentation\nmethod, despite its simplicity. Moreover, the translation quality is further\nimproved by the proposed method, when combined with back-translation.\n",
        "id": 43
    },
    {
        "title": "Learning to Share by Masking the Non-shared for Multi-domain Sentiment\n  Classification",
        "summary": "  Multi-domain sentiment classification deals with the scenario where labeled\ndata exists for multiple domains but insufficient for training effective\nsentiment classifiers that work across domains. Thus, fully exploiting\nsentiment knowledge shared across domains is crucial for real world\napplications. While many existing works try to extract domain-invariant\nfeatures in high-dimensional space, such models fail to explicitly distinguish\nbetween shared and private features at text-level, which to some extent lacks\ninterpretablity. Based on the assumption that removing domain-related tokens\nfrom texts would help improve their domain-invariance, we instead first\ntransform original sentences to be domain-agnostic. To this end, we propose the\nBertMasker network which explicitly masks domain-related words from texts,\nlearns domain-invariant sentiment features from these domain-agnostic texts,\nand uses those masked words to form domain-aware sentence representations.\nEmpirical experiments on a well-adopted multiple domain sentiment\nclassification dataset demonstrate the effectiveness of our proposed model on\nboth multi-domain sentiment classification and cross-domain settings, by\nincreasing the accuracy by 0.94% and 1.8% respectively. Further analysis on\nmasking proves that removing those domain-related and sentiment irrelevant\ntokens decreases texts' domain distinction, resulting in the performance\ndegradation of a BERT-based domain classifier by over 12%.\n",
        "id": 44
    },
    {
        "title": "Revisiting Few-shot Relation Classification: Evaluation Data and\n  Classification Schemes",
        "summary": "  We explore Few-Shot Learning (FSL) for Relation Classification (RC). Focusing\non the realistic scenario of FSL, in which a test instance might not belong to\nany of the target categories (none-of-the-above, aka NOTA), we first revisit\nthe recent popular dataset structure for FSL, pointing out its unrealistic data\ndistribution. To remedy this, we propose a novel methodology for deriving more\nrealistic few-shot test data from available datasets for supervised RC, and\napply it to the TACRED dataset. This yields a new challenging benchmark for FSL\nRC, on which state of the art models show poor performance. Next, we analyze\nclassification schemes within the popular embedding-based nearest-neighbor\napproach for FSL, with respect to constraints they impose on the embedding\nspace. Triggered by this analysis we propose a novel classification scheme, in\nwhich the NOTA category is represented as learned vectors, shown empirically to\nbe an appealing option for FSL.\n",
        "id": 45
    },
    {
        "title": "Minimal Supervision for Morphological Inflection",
        "summary": "  Neural models for the various flavours of morphological inflection tasks have\nproven to be extremely accurate given ample labeled data -- data that may be\nslow and costly to obtain. In this work we aim to overcome this annotation\nbottleneck by bootstrapping labeled data from a seed as little as {\\em five}\nlabeled paradigms, accompanied by a large bulk of unlabeled text. Our approach\nexploits different kinds of regularities in morphological systems in a\ntwo-phased setup, where word tagging based on {\\em analogies} is followed by\nword pairing based on {\\em distances}. We experiment with the Paradigm Cell\nFilling Problem over eight typologically different languages, and find that, in\nlanguages with relatively simple morphology, orthographic regularities on their\nown allow inflection models to achieve respectable accuracy. Combined\northographic and semantic regularities alleviate difficulties with particularly\ncomplex morpho-phonological systems. Our results suggest that hand-crafting\nmany tagged examples might be an unnecessary effort. However, more work is\nneeded in order to address rarely used forms.\n",
        "id": 46
    },
    {
        "title": "Multilingual and Cross-Lingual Intent Detection from Spoken Data",
        "summary": "  We present a systematic study on multilingual and cross-lingual intent\ndetection from spoken data. The study leverages a new resource put forth in\nthis work, termed MInDS-14, a first training and evaluation resource for the\nintent detection task with spoken data. It covers 14 intents extracted from a\ncommercial system in the e-banking domain, associated with spoken examples in\n14 diverse language varieties. Our key results indicate that combining machine\ntranslation models with state-of-the-art multilingual sentence encoders (e.g.,\nLaBSE) can yield strong intent detectors in the majority of target languages\ncovered in MInDS-14, and offer comparative analyses across different axes:\ne.g., zero-shot versus few-shot learning, translation direction, and impact of\nspeech recognition. We see this work as an important step towards more\ninclusive development and evaluation of multilingual intent detectors from\nspoken data, in a much wider spectrum of languages compared to prior work.\n",
        "id": 47
    },
    {
        "title": "The Impact of ASR on the Automatic Analysis of Linguistic Complexity and\n  Sophistication in Spontaneous L2 Speech",
        "summary": "  In recent years, automated approaches to assessing linguistic complexity in\nsecond language (L2) writing have made significant progress in gauging learner\nperformance, predicting human ratings of the quality of learner productions,\nand benchmarking L2 development. In contrast, there is comparatively little\nwork in the area of speaking, particularly with respect to fully automated\napproaches to assessing L2 spontaneous speech. While the importance of a\nwell-performing ASR system is widely recognized, little research has been\nconducted to investigate the impact of its performance on subsequent automatic\ntext analysis. In this paper, we focus on this issue and examine the impact of\nusing a state-of-the-art ASR system for subsequent automatic analysis of\nlinguistic complexity in spontaneously produced L2 speech. A set of 30 selected\nmeasures were considered, falling into four categories: syntactic, lexical,\nn-gram frequency, and information-theoretic measures. The agreement between the\nscores for these measures obtained on the basis of ASR-generated vs. manual\ntranscriptions was determined through correlation analysis. A more differential\neffect of ASR performance on specific types of complexity measures when\ncontrolling for task type effects is also presented.\n",
        "id": 48
    },
    {
        "title": "The Topic Confusion Task: A Novel Scenario for Authorship Attribution",
        "summary": "  Authorship attribution is the problem of identifying the most plausible\nauthor of an anonymous text from a set of candidate authors. Researchers have\ninvestigated same-topic and cross-topic scenarios of authorship attribution,\nwhich differ according to whether new, unseen topics are used in the testing\nphase. However, neither scenario allows us to explain whether errors are caused\nby a failure to capture authorship writing style or by a topic shift. Motivated\nby this, we propose the \\emph{topic confusion} task where we switch the\nauthor-topic configuration between the training and testing sets. This setup\nallows us to distinguish two types of errors: those caused by the topic shift\nand those caused by the features' inability to capture the writing styles. We\nshow that stylometric features with part-of-speech tags are the least\nsusceptible to topic variations. We further show that combining them with other\nfeatures leads to significantly lower topic confusion and higher attribution\naccuracy. Finally, we show that pretrained language models such as BERT and\nRoBERTa perform poorly on this task and are surpassed by simple features such\nas word-level $n$-grams.\n",
        "id": 49
    },
    {
        "title": "The challenges of temporal alignment on Twitter during crises",
        "summary": "  Language use changes over time, and this impacts the effectiveness of NLP\nsystems. This phenomenon is even more prevalent in social media data during\ncrisis events where meaning and frequency of word usage may change over the\ncourse of days. Contextual language models fail to adapt temporally,\nemphasizing the need for temporal adaptation in models which need to be\ndeployed over an extended period of time. While existing approaches consider\ndata spanning large periods of time (from years to decades), shorter time spans\nare critical for crisis data. We quantify temporal degradation for this\nscenario and propose methods to cope with performance loss by leveraging\ntechniques from domain adaptation. To the best of our knowledge, this is the\nfirst effort to explore effects of rapid language change driven by adversarial\nadaptations, particularly during natural and human-induced disasters. Through\nextensive experimentation on diverse crisis datasets, we analyze under what\nconditions our approaches outperform strong baselines while highlighting the\ncurrent limitations of temporal adaptation methods in scenarios where access to\nunlabeled data is scarce.\n",
        "id": 50
    },
    {
        "title": "Multi-Perspective Abstractive Answer Summarization",
        "summary": "  Community Question Answering (CQA) forums such as Stack Overflow and Yahoo!\nAnswers contain a rich resource of answers to a wide range of questions. Each\nquestion thread can receive a large number of answers with different\nperspectives. The goal of multi-perspective answer summarization is to produce\na summary that includes all perspectives of the answer. A major obstacle for\nmulti-perspective, abstractive answer summarization is the absence of a dataset\nto provide supervision for producing such summaries. This work introduces a\nnovel dataset creation method to automatically create multi-perspective,\nbullet-point abstractive summaries from an existing CQA forum. Supervision\nprovided by this dataset trains models to inherently produce multi-perspective\nsummaries. Additionally, to train models to output more diverse, faithful\nanswer summaries while retaining multiple perspectives, we propose a\nmulti-reward optimization technique coupled with a sentence-relevance\nprediction multi-task loss. Our methods demonstrate improved coverage of\nperspectives and faithfulness as measured by automatic and human evaluations\ncompared to a strong baseline.\n",
        "id": 51
    },
    {
        "title": "DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages",
        "summary": "  Word meaning is notoriously difficult to capture, both synchronically and\ndiachronically. In this paper, we describe the creation of the largest resource\nof graded contextualized, diachronic word meaning annotation in four different\nlanguages, based on 100,000 human semantic proximity judgments. We thoroughly\ndescribe the multi-round incremental annotation process, the choice for a\nclustering algorithm to group usages into senses, and possible - diachronic and\nsynchronic - uses for this dataset.\n",
        "id": 52
    },
    {
        "title": "Crossing the Conversational Chasm: A Primer on Natural Language\n  Processing for Multilingual Task-Oriented Dialogue Systems",
        "summary": "  In task-oriented dialogue (ToD), a user holds a conversation with an\nartificial agent to complete a concrete task. Although this technology\nrepresents one of the central objectives of AI and has been the focus of ever\nmore intense research and development efforts, it is currently limited to a few\nnarrow domains (e.g., food ordering, ticket booking) and a handful of languages\n(e.g., English, Chinese). This work provides an extensive overview of existing\nmethods and resources in multilingual ToD as an entry point to this exciting\nand emerging field. We find that the most critical factor preventing the\ncreation of truly multilingual ToD systems is the lack of datasets in most\nlanguages for both training and evaluation. In fact, acquiring annotations or\nhuman feedback for each component of modular systems or for data-hungry\nend-to-end systems is expensive and tedious. Hence, state-of-the-art approaches\nto multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer\nfrom resource-rich languages (almost exclusively English), either by means of\nmachine translation or multilingual representations. These approaches are\ncurrently viable only for typologically similar languages and languages with\nparallel / monolingual corpora available. On the other hand, their\neffectiveness beyond these boundaries is doubtful or hard to assess due to the\nlack of linguistically diverse benchmarks (especially for natural language\ngeneration and end-to-end evaluation). To overcome this limitation, we draw\nparallels between components of the ToD pipeline and other NLP tasks, which can\ninspire solutions for learning in low-resource scenarios. Finally, we list\nadditional challenges that multilinguality poses for related areas (such as\nspeech and human-centred evaluation), and indicate future directions that hold\npromise to further expand language coverage and dialogue capabilities of\ncurrent ToD systems.\n",
        "id": 53
    },
    {
        "title": "Sentence Alignment with Parallel Documents Facilitates Biomedical\n  Machine Translation",
        "summary": "  Objective: Today's neural machine translation (NMT) can achieve near\nhuman-level translation quality and greatly facilitates international\ncommunications, but the lack of parallel corpora poses a key problem to the\ndevelopment of translation systems for highly specialized domains, such as\nbiomedicine. This work presents an unsupervised algorithm for deriving parallel\ncorpora from document-level translations by using sentence alignment and\nexplores how training materials affect the performance of biomedical NMT\nsystems. Materials and Methods: Document-level translations are mixed to train\nbilingual word embeddings (BWEs) for the evaluation of cross-lingual word\nsimilarity, and sentence distance is defined by combining semantic and\npositional similarities of the sentences. The alignment of sentences is\nformulated as an extended earth mover's distance problem. A Chinese-English\nbiomedical parallel corpus is derived with the proposed algorithm using\nbilingual articles from UpToDate and translations of PubMed abstracts, which is\nthen used for the training and evaluation of NMT. Results: On two manually\naligned translation datasets, the proposed algorithm achieved accurate sentence\nalignment in the 1-to-1 cases and outperformed competing algorithms in the\nmany-to-many cases. The NMT model fine-tuned on biomedical data significantly\nimproved the in-domain translation quality (zh-en: +17.72 BLEU; en-zh: +17.02\nBLEU). Both the size of the training data and the combination of different\ncorpora can significantly affect the model's performance. Conclusion: The\nproposed algorithm relaxes the assumption for sentence alignment and\neffectively generates accurate translation pairs that facilitate training high\nquality biomedical NMT models.\n",
        "id": 54
    },
    {
        "title": "XLEnt: Mining a Large Cross-lingual Entity Dataset with\n  Lexical-Semantic-Phonetic Word Alignment",
        "summary": "  Cross-lingual named-entity lexica are an important resource to multilingual\nNLP tasks such as machine translation and cross-lingual wikification. While\nknowledge bases contain a large number of entities in high-resource languages\nsuch as English and French, corresponding entities for lower-resource languages\nare often missing. To address this, we propose Lexical-Semantic-Phonetic Align\n(LSP-Align), a technique to automatically mine cross-lingual entity lexica from\nmined web data. We demonstrate LSP-Align outperforms baselines at extracting\ncross-lingual entity pairs and mine 164 million entity pairs from 120 different\nlanguages aligned with English. We release these cross-lingual entity pairs\nalong with the massively multilingual tagged named entity corpus as a resource\nto the NLP community.\n",
        "id": 55
    },
    {
        "title": "A Stylistic Analysis of Honest Deception: The Case of Seinfeld TV Series\n  Sitcom",
        "summary": "  Language is a powerful tool if used in the correct manner. It is the major\nmode of communication, and using the correct choice of words and styles can\nserve to have a long-lasting impact. Stylistics is the study of the use of\nvarious language styles in communication to pass a message with a bigger impact\nor to communicate indirectly. Stylistic analysis, therefore, is the study of\nthe use of linguistic styles in texts to determine how a style has been used,\nwhat is communicated and how it is communicated. Honest deception is the use of\na choice of words to imply something different from the literal meaning. A\nperson listening or reading a text where honest deception has been used and\nwith a literal understanding may completely miss out on the point. This is\nbecause the issue of honesty and falsehood arises. However, it would be better\nto understand that honest deception is used with the intention of having a\nlasting impact rather than to deceive the readers, viewers or listeners. The\nmajor styles used in honest deception are hyperboles, litotes, irony and\nsarcasm. The Seinfeld Sitcom TV series was a situational TV comedy show aired\nfrom 1990 to 1998. the show attempts to bring to the understanding the daily\nlife of a comedian and how comedian views life experiences and convert them\ninto hilarious jokes. It also shows Jerry's struggle with getting the right\npartner from the many women who come into his life. Reflecting on honest\ndeception in the Seinfeld sitcom TV series, this paper is going to investigate\nhow honest deception has been used in the series, why it has been used and what\nis being communicated. The study is going to use a recapitulative form to give\na better analysis and grouping of the different styles used in honest deception\nthroughout the series.\n",
        "id": 56
    },
    {
        "title": "Who Responded to Whom: The Joint Effects of Latent Topics and Discourse\n  in Conversation Structure",
        "summary": "  Numerous online conversations are produced on a daily basis, resulting in a\npressing need to conversation understanding. As a basis to structure a\ndiscussion, we identify the responding relations in the conversation discourse,\nwhich link response utterances to their initiations. To figure out who\nresponded to whom, here we explore how the consistency of topic contents and\ndependency of discourse roles indicate such interactions, whereas most prior\nwork ignore the effects of latent factors underlying word occurrences. We\npropose a model to learn latent topics and discourse in word distributions, and\npredict pairwise initiation-response links via exploiting topic consistency and\ndiscourse dependency. Experimental results on both English and Chinese\nconversations show that our model significantly outperforms the previous state\nof the arts, such as 79 vs. 73 MRR on Chinese customer service dialogues. We\nfurther probe into our outputs and shed light on how topics and discourse\nindicate conversational user interactions.\n",
        "id": 57
    },
    {
        "title": "Emotion Classification in a Resource Constrained Language Using\n  Transformer-based Approach",
        "summary": "  Although research on emotion classification has significantly progressed in\nhigh-resource languages, it is still infancy for resource-constrained languages\nlike Bengali. However, unavailability of necessary language processing tools\nand deficiency of benchmark corpora makes the emotion classification task in\nBengali more challenging and complicated. This work proposes a\ntransformer-based technique to classify the Bengali text into one of the six\nbasic emotions: anger, fear, disgust, sadness, joy, and surprise. A Bengali\nemotion corpus consists of 6243 texts is developed for the classification task.\nExperimentation carried out using various machine learning (LR, RF, MNB, SVM),\ndeep neural networks (CNN, BiLSTM, CNN+BiLSTM) and transformer (Bangla-BERT,\nm-BERT, XLM-R) based approaches. Experimental outcomes indicate that XLM-R\noutdoes all other techniques by achieving the highest weighted $f_1$-score of\n$69.73\\%$ on the test data. The dataset is publicly available at\nhttps://github.com/omar-sharif03/NAACL-SRW-2021.\n",
        "id": 58
    },
    {
        "title": "UPB at SemEval-2021 Task 5: Virtual Adversarial Training for Toxic Spans\n  Detection",
        "summary": "  The real-world impact of polarization and toxicity in the online sphere\nmarked the end of 2020 and the beginning of this year in a negative way.\nSemeval-2021, Task 5 - Toxic Spans Detection is based on a novel annotation of\na subset of the Jigsaw Unintended Bias dataset and is the first language\ntoxicity detection task dedicated to identifying the toxicity-level spans. For\nthis task, participants had to automatically detect character spans in short\ncomments that render the message as toxic. Our model considers applying Virtual\nAdversarial Training in a semi-supervised setting during the fine-tuning\nprocess of several Transformer-based models (i.e., BERT and RoBERTa), in\ncombination with Conditional Random Fields. Our approach leads to performance\nimprovements and more robust models, enabling us to achieve an F1-score of\n65.73% in the official submission and an F1-score of 66.13% after further\ntuning during post-evaluation.\n",
        "id": 59
    },
    {
        "title": "AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages\n  with Adversarial Examples",
        "summary": "  Capturing word meaning in context and distinguishing between correspondences\nand variations across languages is key to building successful multilingual and\ncross-lingual text representation models. However, existing multilingual\nevaluation datasets that evaluate lexical semantics \"in-context\" have various\nlimitations. In particular, 1) their language coverage is restricted to\nhigh-resource languages and skewed in favor of only a few language families and\nareas, 2) a design that makes the task solvable via superficial cues, which\nresults in artificially inflated (and sometimes super-human) performances of\npretrained encoders, on many target languages, which limits their usefulness\nfor model probing and diagnostics, and 3) little support for cross-lingual\nevaluation. In order to address these gaps, we present AM2iCo (Adversarial and\nMultilingual Meaning in Context), a wide-coverage cross-lingual and\nmultilingual evaluation set; it aims to faithfully assess the ability of\nstate-of-the-art (SotA) representation models to understand the identity of\nword meaning in cross-lingual contexts for 14 language pairs. We conduct a\nseries of experiments in a wide range of setups and demonstrate the challenging\nnature of AM2iCo. The results reveal that current SotA pretrained encoders\nsubstantially lag behind human performance, and the largest gaps are observed\nfor low-resource languages and languages dissimilar to English.\n",
        "id": 60
    },
    {
        "title": "Customized determination of stop words using Random Matrix Theory\n  approach",
        "summary": "  The distances between words calculated in word units are studied and compared\nwith the distributions of the Random Matrix Theory (RMT). It is found that the\ndistribution of distance between the same words can be well described by the\nsingle-parameter Brody distribution. Using the Brody distribution fit, we found\nthat the distance between given words in a set of texts can show mixed\ndynamics, coexisting regular and chaotic regimes. It is found that\ndistributions correctly fitted by the Brody distribution with a certain\ngoodness of the fit threshold can be identifid as stop words, usually\nconsidered as the uninformative part of the text. By applying various threshold\nvalues for the goodness of fit, we can extract uninformative words from the\ntexts under analysis to the desired extent. On this basis we formulate a fully\nagnostic recipe that can be used in the creation of a customized set of stop\nwords for texts in any language based on words.\n",
        "id": 61
    },
    {
        "title": "Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training",
        "summary": "  Pre-trained multilingual language encoders, such as multilingual BERT and\nXLM-R, show great potential for zero-shot cross-lingual transfer. However,\nthese multilingual encoders do not precisely align words and phrases across\nlanguages. Especially, learning alignments in the multilingual embedding space\nusually requires sentence-level or word-level parallel corpora, which are\nexpensive to be obtained for low-resource languages. An alternative is to make\nthe multilingual encoders more robust; when fine-tuning the encoder using\ndownstream task, we train the encoder to tolerate noise in the contextual\nembedding spaces such that even if the representations of different languages\nare not aligned well, the model can still achieve good performance on zero-shot\ncross-lingual transfer. In this work, we propose a learning strategy for\ntraining robust models by drawing connections between adversarial examples and\nthe failure cases of zero-shot cross-lingual transfer. We adopt two widely used\nrobust training methods, adversarial training and randomized smoothing, to\ntrain the desired robust model. The experimental results demonstrate that\nrobust training improves zero-shot cross-lingual transfer on text\nclassification tasks. The improvement is more significant in the generalized\ncross-lingual transfer setting, where the pair of input sentences belong to two\ndifferent languages.\n",
        "id": 62
    },
    {
        "title": "Competency Problems: On Finding and Removing Artifacts in Language Data",
        "summary": "  Much recent work in NLP has documented dataset artifacts, bias, and spurious\ncorrelations between input features and output labels. However, how to tell\nwhich features have \"spurious\" instead of legitimate correlations is typically\nleft unspecified. In this work we argue that for complex language understanding\ntasks, all simple feature correlations are spurious, and we formalize this\nnotion into a class of problems which we call competency problems. For example,\nthe word \"amazing\" on its own should not give information about a sentiment\nlabel independent of the context in which it appears, which could include\nnegation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of\ncreating data for competency problems when human bias is taken into account,\nshowing that realistic datasets will increasingly deviate from competency\nproblems as dataset size increases. This analysis gives us a simple statistical\ntest for dataset artifacts, which we use to show more subtle biases than were\ndescribed in prior work, including demonstrating that models are\ninappropriately affected by these less extreme biases. Our theoretical\ntreatment of this problem also allows us to analyze proposed solutions, such as\nmaking local edits to dataset instances, and to give recommendations for future\ndata collection and model design efforts that target competency problems.\n",
        "id": 63
    },
    {
        "title": "Learning from Noisy Labels for Entity-Centric Information Extraction",
        "summary": "  Recent information extraction approaches have relied on training deep neural\nmodels. However, such models can easily overfit noisy labels and suffer from\nperformance degradation. While it is very costly to filter noisy labels in\nlarge learning resources, recent studies show that such labels take more\ntraining steps to be memorized and are more frequently forgotten than clean\nlabels, therefore are identifiable in training. Motivated by such properties,\nwe propose a simple co-regularization framework for entity-centric information\nextraction, which consists of several neural models with identical structures\nbut different parameter initialization. These models are jointly optimized with\nthe task-specific losses and are regularized to generate similar predictions\nbased on an agreement loss, which prevents overfitting on noisy labels.\nExtensive experiments on two widely used but noisy benchmarks for information\nextraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.\nWe release our code to the community for future research.\n",
        "id": 64
    },
    {
        "title": "Monotonicity Marking from Universal Dependency Trees",
        "summary": "  Dependency parsing is a tool widely used in the field of Natural language\nprocessing and computational linguistics. However, there is hardly any work\nthat connects dependency parsing to monotonicity, which is an essential part of\nlogic and linguistic semantics. In this paper, we present a system that\nautomatically annotates monotonicity information based on Universal Dependency\nparse trees. Our system utilizes surface-level monotonicity facts about\nquantifiers, lexical items, and token-level polarity information. We compared\nour system's performance with existing systems in the literature, including\nNatLog and ccg2mono, on a small evaluation dataset. Results show that our\nsystem outperforms NatLog and ccg2mono.\n",
        "id": 65
    },
    {
        "title": "Characterizing Idioms: Conventionality and Contingency",
        "summary": "  Idioms are unlike most phrases in two important ways. First, the words in an\nidiom have non-canonical meanings. Second, the non-canonical meanings of words\nin an idiom are contingent on the presence of other words in the idiom.\nLinguistic theories differ on whether these properties depend on one another,\nas well as whether special theoretical machinery is needed to accommodate\nidioms. We define two measures that correspond to the properties above, and we\nimplement them using BERT (Devlin et al., 2019) and XLNet(Yang et al., 2019).\nWe show that idioms fall at the expected intersection of the two dimensions,\nbut that the dimensions themselves are not correlated. Our results suggest that\nspecial machinery to handle idioms may not be warranted.\n",
        "id": 66
    },
    {
        "title": "Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language\n  Models",
        "summary": "  Numerous works have analyzed biases in vision and pre-trained language models\nindividually - however, less attention has been paid to how these biases\ninteract in multimodal settings. This work extends text-based bias analysis\nmethods to investigate multimodal language models, and analyzes intra- and\ninter-modality associations and biases learned by these models. Specifically,\nwe demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often\npreferring to reinforce a stereotype over faithfully describing the visual\nscene. We demonstrate these findings on a controlled case-study and extend them\nfor a larger set of stereotypically gendered entities.\n",
        "id": 67
    },
    {
        "title": "Generating Related Work",
        "summary": "  Communicating new research ideas involves highlighting similarities and\ndifferences with past work. Authors write fluent, often long sections to survey\nthe distinction of a new paper with related work. In this work we model\ngenerating related work sections while being cognisant of the motivation behind\nciting papers. Our content planning model generates a tree of cited papers\nbefore a surface realization model lexicalizes this skeleton. Our model\noutperforms several strong state-of-the-art summarization and multi-document\nsummarization models on generating related work on an ACL Anthology (AA) based\ndataset which we contribute.\n",
        "id": 68
    },
    {
        "title": "When Does Pretraining Help? Assessing Self-Supervised Learning for Law\n  and the CaseHOLD Dataset",
        "summary": "  While self-supervised learning has made rapid advances in natural language\nprocessing, it remains unclear when researchers should engage in\nresource-intensive domain-specific pretraining (domain pretraining). The law,\npuzzlingly, has yielded few documented instances of substantial gains to domain\npretraining in spite of the fact that legal language is widely seen to be\nunique. We hypothesize that these existing results stem from the fact that\nexisting legal NLP tasks are too easy and fail to meet conditions for when\ndomain pretraining can help. To address this, we first present CaseHOLD (Case\nHoldings On Legal Decisions), a new dataset comprised of over 53,000+ multiple\nchoice questions to identify the relevant holding of a cited case. This dataset\npresents a fundamental task to lawyers and is both legally meaningful and\ndifficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second,\nwe assess performance gains on CaseHOLD and existing legal NLP datasets. While\na Transformer architecture (BERT) pretrained on a general corpus (Google Books\nand Wikipedia) improves performance, domain pretraining (using corpus of\napproximately 3.5M decisions across all courts in the U.S. that is larger than\nBERT's) with a custom legal vocabulary exhibits the most substantial\nperformance gains with CaseHOLD (gain of 7.2% on F1, representing a 12%\nimprovement on BERT) and consistent performance gains across two other legal\ntasks. Third, we show that domain pretraining may be warranted when the task\nexhibits sufficient similarity to the pretraining corpus: the level of\nperformance increase in three legal tasks was directly tied to the domain\nspecificity of the task. Our findings inform when researchers should engage\nresource-intensive pretraining and show that Transformer-based architectures,\ntoo, learn embeddings suggestive of distinct legal language.\n",
        "id": 69
    },
    {
        "title": "Guilt by Association: Emotion Intensities in Lexical Representations",
        "summary": "  What do word vector representations reveal about the emotions associated with\nwords? In this study, we consider the task of estimating word-level emotion\nintensity scores for specific emotions, exploring unsupervised, supervised, and\nfinally a self-supervised method of extracting emotional associations from word\nvector representations. Overall, we find that word vectors carry substantial\npotential for inducing fine-grained emotion intensity scores, showing a far\nhigher correlation with human ground truth ratings than achieved by\nstate-of-the-art emotion lexicons.\n",
        "id": 70
    },
    {
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "summary": "  In this work, we explore \"prompt tuning\", a simple yet effective mechanism\nfor learning \"soft prompts\" to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod \"closes the gap\" and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed \"prefix tuning\" of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.\n",
        "id": 71
    },
    {
        "title": "MT6: Multilingual Pretrained Text-to-Text Transformer with Translation\n  Pairs",
        "summary": "  Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive\nmonolingual texts, which has shown promising results on many cross-lingual\ntasks. In this paper, we improve multilingual text-to-text transfer Transformer\nwith translation pairs (mT6). Specifically, we explore three cross-lingual\ntext-to-text pre-training tasks, namely, machine translation, translation pair\nspan corruption, and translation span corruption. In addition, we propose a\npartially non-autoregressive objective for text-to-text pre-training. We\nevaluate the methods on eight multilingual benchmark datasets, including\nsentence classification, named entity recognition, question answering, and\nabstractive summarization. Experimental results show that the proposed mT6\nimproves cross-lingual transferability over mT5.\n",
        "id": 72
    },
    {
        "title": "Knowledge Neurons in Pretrained Transformers",
        "summary": "  Large-scale pretrained language models are surprisingly good at recalling\nfactual knowledge presented in the training corpus. In this paper, we present\npreliminary studies on how factual knowledge is stored in pretrained\nTransformers by introducing the concept of knowledge neurons. Specifically, we\nexamine the fill-in-the-blank cloze task for BERT. Given a relational fact, we\npropose a knowledge attribution method to identify the neurons that express the\nfact. We find that the activation of such knowledge neurons is positively\ncorrelated to the expression of their corresponding facts. In our case studies,\nwe attempt to leverage knowledge neurons to edit (such as update, and erase)\nspecific factual knowledge without fine-tuning. Our results shed light on\nunderstanding the storage of knowledge within pretrained Transformers. The code\nis available at https://github.com/Hunter-DDM/knowledge-neurons.\n",
        "id": 73
    },
    {
        "title": "Intent Features for Rich Natural Language Understanding",
        "summary": "  Complex natural language understanding modules in dialog systems have a\nricher understanding of user utterances, and thus are critical in providing a\nbetter user experience. However, these models are often created from scratch,\nfor specific clients and use cases, and require the annotation of large\ndatasets. This encourages the sharing of annotated data across multiple\nclients. To facilitate this we introduce the idea of intent features: domain\nand topic agnostic properties of intents that can be learned from the syntactic\ncues only, and hence can be shared. We introduce a new neural network\narchitecture, the Global-Local model, that shows significant improvement over\nstrong baselines for identifying these features in a deployed, multi-intent\nnatural language understanding module, and, more generally, in a classification\nsetting where a part of an utterance has to be classified utilizing the whole\ncontext.\n",
        "id": 74
    },
    {
        "title": "Simple and Efficient ways to Improve REALM",
        "summary": "  Dense retrieval has been shown to be effective for retrieving relevant\ndocuments for Open Domain QA, surpassing popular sparse retrieval methods like\nBM25. REALM (Guu et al., 2020) is an end-to-end dense retrieval system that\nrelies on MLM based pretraining for improved downstream QA efficiency across\nmultiple datasets. We study the finetuning of REALM on various QA tasks and\nexplore the limits of various hyperparameter and supervision choices. We find\nthat REALM was significantly undertrained when finetuning and simple\nimprovements in the training, supervision, and inference setups can\nsignificantly benefit QA results and exceed the performance of other models\npublished post it. Our best model, REALM++, incorporates all the best working\nfindings and achieves significant QA accuracy improvements over baselines\n(~5.5% absolute accuracy) without any model design changes. Additionally,\nREALM++ matches the performance of large Open Domain QA models which have 3x\nmore parameters demonstrating the efficiency of the setup.\n",
        "id": 75
    },
    {
        "title": "PaCo: Preconditions Attributed to Commonsense Knowledge",
        "summary": "  Humans can seamlessly reason with circumstantial preconditions of commonsense\nknowledge. We understand that a glass is used for drinking water, unless the\nglass is broken or the water is toxic. Despite state-of-the-art (SOTA) language\nmodels' (LMs) impressive performance on inferring commonsense knowledge, it is\nunclear whether they understand the circumstantial preconditions. To address\nthis gap, we propose a novel challenge of reasoning with circumstantial\npreconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand\npreconditions of commonsense statements expressed in natural language. Based on\nthis dataset, we create three canonical evaluation tasks and use them to\nexamine the capability of existing LMs to understand situational preconditions.\nOur results reveal a 10-30% gap between machine and human performance on our\ntasks, which shows that reasoning with preconditions is an open challenge.\n",
        "id": 76
    },
    {
        "title": "Embedding-Enhanced Giza++: Improving Alignment in Low- and High-\n  Resource Scenarios Using Embedding Space Geometry",
        "summary": "  A popular natural language processing task decades ago, word alignment has\nbeen dominated until recently by GIZA++, a statistical method based on the\n30-year-old IBM models. New methods that outperform GIZA++ primarily rely on\nlarge machine translation models, massively multilingual language models, or\nsupervision from GIZA++ alignments itself. We introduce Embedding-Enhanced\nGIZA++, and outperform GIZA++ without any of the aforementioned factors. Taking\nadvantage of monolingual embedding spaces of source and target language only,\nwe exceed GIZA++'s performance in every tested scenario for three languages\npairs. In the lowest-resource setting, we outperform GIZA++ by 8.5, 10.9, and\n12 AER for Ro-En, De-En, and En-Fr, respectively. We release our code at\nhttps://github.com/kellymarchisio/ee-giza.\n",
        "id": 77
    },
    {
        "title": "News Meets Microblog: Hashtag Annotation via Retriever-Generator",
        "summary": "  Hashtag annotation for microblog posts has been recently formulated as a\nsequence generation problem to handle emerging hashtags that are unseen in the\ntraining set. The state-of-the-art method leverages conversations initiated by\nposts to enrich contextual information for the short posts. However, it is\nunrealistic to assume the existence of conversations before the hashtag\nannotation itself. Therefore, we propose to leverage news articles published\nbefore the microblog post to generate hashtags following a Retriever-Generator\nframework. Extensive experiments on English Twitter datasets demonstrate\nsuperior performance and significant advantages of leveraging news articles to\ngenerate hashtags.\n",
        "id": 78
    },
    {
        "title": "Extract, Denoise and Enforce: Evaluating and Improving Concept\n  Preservation for Text-to-Text Generation",
        "summary": "  Prior studies on text-to-text generation typically assume that the model\ncould figure out what to attend to in the input and what to include in the\noutput via seq2seq learning, with only the parallel training data and no\nadditional guidance. However, it remains unclear whether current models can\npreserve important concepts in the source input, as seq2seq learning does not\nhave explicit focus on the concepts and commonly used evaluation metrics also\ntreat concepts equally important as other tokens. In this paper, we present a\nsystematic analysis that studies whether current seq2seq models, especially\npre-trained language models, are good enough for preserving important input\nconcepts and to what extent explicitly guiding generation with the concepts as\nlexical constraints is beneficial. We answer the above questions by conducting\nextensive analytical experiments on four representative text-to-text generation\ntasks. Based on the observations, we then propose a simple yet effective\nframework to automatically extract, denoise, and enforce important input\nconcepts as lexical constraints. This new method performs comparably or better\nthan its unconstrained counterpart on automatic metrics, demonstrates higher\ncoverage for concept preservation, and receives better ratings in the human\nevaluation. Our code is available at https://github.com/morningmoni/EDE.\n",
        "id": 79
    },
    {
        "title": "AmericasNLI: Evaluating Zero-shot Natural Language Understanding of\n  Pretrained Multilingual Models in Truly Low-resource Languages",
        "summary": "  Pretrained multilingual models are able to perform cross-lingual transfer in\na zero-shot setting, even for languages unseen during pretraining. However,\nprior work evaluating performance on unseen languages has largely been limited\nto low-level, syntactic tasks, and it remains unclear if zero-shot learning of\nhigh-level, semantic tasks is possible for unseen languages. To explore this\nquestion, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018)\nto 10 indigenous languages of the Americas. We conduct experiments with XLM-R,\ntesting multiple zero-shot and translation-based approaches. Additionally, we\nexplore model adaptation via continued pretraining and provide an analysis of\nthe dataset by considering hypothesis-only models. We find that XLM-R's\nzero-shot performance is poor for all 10 languages, with an average performance\nof 38.62%. Continued pretraining offers improvements, with an average accuracy\nof 44.05%. Surprisingly, training on poorly translated data by far outperforms\nall other methods with an accuracy of 48.72%.\n",
        "id": 80
    },
    {
        "title": "Revealing Persona Biases in Dialogue Systems",
        "summary": "  Dialogue systems in the form of chatbots and personal assistants are being\nincreasingly integrated into people's lives. Modern dialogue systems may\nconsider adopting anthropomorphic personas, mimicking societal demographic\ngroups to appear more approachable and trustworthy to users. However, the\nadoption of a persona can result in the adoption of biases. In this paper, we\npresent the first large-scale study on persona biases in dialogue systems and\nconduct analyses on personas of different social classes, sexual orientations,\nraces, and genders. We define persona biases as harmful differences in\nresponses (e.g., varying levels of offensiveness, agreement with harmful\nstatements) generated from adopting different demographic personas.\nFurthermore, we introduce an open-source framework, UnitPersonaBias, to explore\nand aggregate persona biases in dialogue systems. By analyzing the Blender and\nDialoGPT dialogue systems, we observe that adopting personas can actually\ndecrease harmful responses, compared to not using any personas. Additionally,\nwe find that persona choices can affect the degree of harms in generated\nresponses and thus should be systematically evaluated before deployment. We\nalso analyze how personas can result in different amounts of harm towards\nspecific demographics.\n",
        "id": 81
    },
    {
        "title": "Learning with Instance Bundles for Reading Comprehension",
        "summary": "  When training most modern reading comprehension models, all the questions\nassociated with a context are treated as being independent from each other.\nHowever, closely related questions and their corresponding answers are not\nindependent, and leveraging these relationships could provide a strong\nsupervision signal to a model. Drawing on ideas from contrastive estimation, we\nintroduce several new supervision techniques that compare question-answer\nscores across multiple related instances. Specifically, we normalize these\nscores across various neighborhoods of closely contrasting questions and/or\nanswers, adding another cross entropy loss term that is used in addition to\ntraditional maximum likelihood estimation. Our techniques require bundles of\nrelated question-answer pairs, which we can either mine from within existing\ndata or create using various automated heuristics. We empirically demonstrate\nthe effectiveness of training with instance bundles on two datasets -- HotpotQA\nand ROPES -- showing up to 11% absolute gains in accuracy.\n",
        "id": 82
    },
    {
        "title": "CEAR: Cross-Entity Aware Reranker for Knowledge Base Completion",
        "summary": "  Pre-trained language models (LMs) like BERT have shown to store factual\nknowledge about the world. This knowledge can be used to augment the\ninformation present in Knowledge Bases, which tend to be incomplete. However,\nprior attempts at using BERT for task of Knowledge Base Completion (KBC)\nresulted in performance worse than embedding based techniques that rely only on\nthe graph structure. In this work we develop a novel model, Cross-Entity Aware\nReranker (CEAR), that uses BERT to re-rank the output of existing KBC models\nwith cross-entity attention. Unlike prior work that scores each entity\nindependently, CEAR uses BERT to score the entities together, which is\neffective for exploiting its factual knowledge. CEAR achieves a new state of\nart for the OLPBench dataset.\n",
        "id": 83
    },
    {
        "title": "Go Forth and Prosper: Language Modeling with Ancient Textual History",
        "summary": "  We introduce a technique for improving document-level language models (LM) by\nleveraging \"ancient history\": text that is outside the LM's current context\nwindow. We learn an auxiliary function to select spans from the ancient history\nwhich can help the LM to predict future text. The selected text spans are then\ncopied directly into the LM's context window, replacing less predictive spans.\nThis method can improve perplexity of pretrained LMs with no updates to the\nLM's own parameters. We further observe that an auxiliary function trained in a\nspecific textual domain like Wikipedia will also work in a substantially\ndifferent domain such as scientific publications. With this technique we see a\n7 percent perplexity reduction on Wikipedia articles, and a 12 percent\nperplexity reduction on scientific texts.\n",
        "id": 84
    },
    {
        "title": "Generative Context Pair Selection for Multi-hop Question Answering",
        "summary": "  Compositional reasoning tasks like multi-hop question answering, require\nmaking latent decisions to get the final answer, given a question. However,\ncrowdsourced datasets often capture only a slice of the underlying task\ndistribution, which can induce unanticipated biases in models performing\ncompositional reasoning. Furthermore, discriminatively trained models exploit\nsuch biases to get a better held-out performance, without learning the right\nway to reason, as they do not necessitate paying attention to the question\nrepresentation (conditioning variable) in its entirety, to estimate the answer\nlikelihood. In this work, we propose a generative context selection model for\nmulti-hop question answering that reasons about how the given question could\nhave been generated given a context pair. While being comparable to the\nstate-of-the-art answering performance, our proposed generative passage\nselection model has a better performance (4.9% higher than baseline) on\nadversarial held-out set which tests robustness of model's multi-hop reasoning\ncapabilities.\n",
        "id": 85
    },
    {
        "title": "Zero-shot Cross-lingual Transfer of Neural Machine Translation with\n  Multilingual Pretrained Encoders",
        "summary": "  Previous work mainly focuses on improving cross-lingual transfer for NLU\ntasks with a multilingual pretrained encoder (MPE), or improving the\nperformance on supervised machine translation with BERT. However, it is\nunder-explored that whether the MPE can help to facilitate the cross-lingual\ntransferability of NMT model. In this paper, we focus on a zero-shot\ncross-lingual transfer task in NMT. In this task, the NMT model is trained with\nparallel dataset of only one language pair and an off-the-shelf MPE, then it is\ndirectly tested on zero-shot language pairs. We propose SixT, a simple yet\neffective model for this task. SixT leverages the MPE with a two-stage training\nschedule and gets further improvement with a position disentangled encoder and\na capacity-enhanced decoder. Using this method, SixT significantly outperforms\nmBART, a pretrained multilingual encoder-decoder model explicitly designed for\nNMT, with an average improvement of 7.1 BLEU on zero-shot any-to-English test\nsets across 14 source languages. Furthermore, with much less training\ncomputation cost and training data, our model achieves better performance on 15\nany-to-English test sets than CRISS and m2m-100, two strong multilingual NMT\nbaselines.\n",
        "id": 86
    },
    {
        "title": "Improving Neural Model Performance through Natural Language Feedback on\n  Their Explanations",
        "summary": "  A class of explainable NLP models for reasoning tasks support their decisions\nby generating free-form or structured explanations, but what happens when these\nsupporting structures contain errors? Our goal is to allow users to\ninteractively correct explanation structures through natural language feedback.\nWe introduce MERCURIE - an interactive system that refines its explanations for\na given reasoning task by getting human feedback in natural language. Our\napproach generates graphs that have 40% fewer inconsistencies as compared with\nthe off-the-shelf system. Further, simply appending the corrected explanation\nstructures to the output leads to a gain of 1.2 points on accuracy on\ndefeasible reasoning across all three domains. We release a dataset of over\n450k graphs for defeasible reasoning generated by our system at\nhttps://tinyurl.com/mercurie .\n",
        "id": 87
    },
    {
        "title": "Constrained Language Models Yield Few-Shot Semantic Parsers",
        "summary": "  We explore the use of large pretrained language models as few-shot semantic\nparsers. The goal in semantic parsing is to generate a structured meaning\nrepresentation given a natural language input. However, language models are\ntrained to generate natural language. To bridge the gap, we use language models\nto paraphrase inputs into a controlled sublanguage resembling English that can\nbe automatically mapped to a target meaning representation. Our results\ndemonstrate that with only a small amount of data and very little code to\nconvert into English-like representations, our blueprint for rapidly\nbootstrapping semantic parsers leads to surprisingly effective performance on\nmultiple community tasks, greatly exceeding baseline methods also trained on\nthe same limited data.\n",
        "id": 88
    },
    {
        "title": "Cross-Attention is All You Need: Adapting Pretrained Transformers for\n  Machine Translation",
        "summary": "  We study the power of cross-attention in the Transformer architecture within\nthe context of transfer learning for machine translation, and extend the\nfindings of studies into cross-attention when training from scratch. We conduct\na series of experiments through fine-tuning a translation model on data where\neither the source or target language has changed. These experiments reveal that\nfine-tuning only the cross-attention parameters is nearly as effective as\nfine-tuning all parameters (i.e., the entire translation model). We provide\ninsights into why this is the case and observe that limiting fine-tuning in\nthis manner yields cross-lingually aligned embeddings. The implications of this\nfinding for researchers and practitioners include a mitigation of catastrophic\nforgetting, the potential for zero-shot translation, and the ability to extend\nmachine translation models to several new language pairs with reduced parameter\nstorage overhead.\n",
        "id": 89
    },
    {
        "title": "On the Sensitivity and Stability of Model Interpretations in NLP",
        "summary": "  Recent years have witnessed the emergence of a variety of post-hoc\ninterpretations that aim to uncover how natural language processing (NLP)\nmodels make predictions. Despite the surge of new interpretation methods, it\nremains an open problem how to define and quantitatively measure the\nfaithfulness of interpretations, i.e., to what extent interpretations reflect\nthe reasoning process by a model. We propose two new criteria, sensitivity and\nstability, that provide complementary notions of faithfulness to the existed\nremoval-based criteria. Our results show that the conclusion for how faithful\ninterpretations are could vary substantially based on different notions.\nMotivated by the desiderata of sensitivity and stability, we introduce a new\nclass of interpretation methods that adopt techniques from adversarial\nrobustness. Empirical results show that our proposed methods are effective\nunder the new criteria and overcome limitations of gradient-based methods on\nremoval-based criteria. Besides text classification, we also apply\ninterpretation methods and metrics to dependency parsing. Our results shed\nlight on understanding the diverse set of interpretations.\n",
        "id": 90
    },
    {
        "title": "Chinese Sentences Similarity via Cross-Attention Based Siamese Network",
        "summary": "  Measuring sentence similarity is a key research area nowadays as it allows\nmachines to better understand human languages. In this paper, we proposed a\nCross-Attention Siamese Network (CATsNet) to carry out the task of learning the\nsemantic meanings of Chinese sentences and comparing the similarity between two\nsentences. This novel model is capable of catching non-local features.\nAdditionally, we also tried to apply the long short-term memory (LSTM) network\nin the model to improve its performance. The experiments were conducted on the\nLCQMC dataset and the results showed that our model could achieve a higher\naccuracy than previous work.\n",
        "id": 91
    },
    {
        "title": "Misinfo Reaction Frames: Reasoning about Readers' Reactions to News\n  Headlines",
        "summary": "  Even to a simple and short news headline, readers react in a multitude of\nways: cognitively (e.g. inferring the writer's intent), emotionally (e.g.\nfeeling distrust), and behaviorally (e.g. sharing the news with their friends).\nSuch reactions are instantaneous and yet complex, as they rely on factors that\ngo beyond interpreting factual content of news. We propose Misinfo Reaction\nFrames (MRF), a pragmatic formalism for modeling how readers might react to a\nnews headline. In contrast to categorical schema, our free-text dimensions\nprovide a more nuanced way of understanding intent beyond being benign or\nmalicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced\ndataset of reactions to over 25k news headlines focusing on global crises: the\nCovid-19 pandemic, climate change, and cancer. Empirical results confirm that\nit is indeed possible for neural models to predict the prominent patterns of\nreaders' reactions to previously unseen news headlines. Additionally, our user\nstudy shows that displaying machine-generated MRF implications alongside news\nheadlines to readers can increase their trust in real news while decreasing\ntheir trust in misinformation. Our work demonstrates the feasibility and\nimportance of pragmatic inferences on news headlines to help enhance AI-guided\nmisinformation detection and mitigation.\n",
        "id": 92
    },
    {
        "title": "Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement\n  Learning",
        "summary": "  Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a\nclassical task for capturing the central idea from a given document. Based on\nSeq2Seq models, the previous reinforcement learning framework on KG tasks\nutilizes the evaluation metrics to further improve the well-trained neural\nmodels. However, these KG evaluation metrics such as $F_1@5$ and $F_1@M$ are\nonly aware of the exact correctness of predictions on phrase-level and ignore\nthe semantic similarities between similar predictions and targets, which\ninhibits the model from learning deep linguistic patterns. In response to this\nproblem, we propose a new fine-grained evaluation metric to improve the RL\nframework, which considers different granularities: token-level $F_1$ score,\nedit distance, duplication, and prediction quantities. On the whole, the new\nframework includes two reward functions: the fine-grained evaluation score and\nthe vanilla $F_1$ score. This framework helps the model identifying some\npartial match phrases which can be further optimized as the exact match ones.\nExperiments on KG benchmarks show that our proposed training framework\noutperforms the previous RL training frameworks among all evaluation scores. In\naddition, our method can effectively ease the synonym problem and generate a\nhigher quality prediction. The source code is available at\n\\url{https://github.com/xuyige/FGRL4KG}.\n",
        "id": 93
    },
    {
        "title": "Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation\n  for Few-shot Learning",
        "summary": "  The ability to continuously expand knowledge over time and utilize it to\nrapidly generalize to new tasks is a key feature of human linguistic\nintelligence. Existing models that pursue rapid generalization to new tasks\n(e.g., few-shot learning methods), however, are mostly trained in a single shot\non fixed datasets, unable to dynamically expand their knowledge; while\ncontinual learning algorithms are not specifically designed for rapid\ngeneralization. We present a new learning setup, Continual Learning of Few-Shot\nLearners (CLIF), to address the challenges of both learning settings in a\nunified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks\narriving sequentially, accumulating knowledge for improved generalization to\nnew tasks, while also retaining performance on the tasks learned earlier. We\nexamine how the generalization ability is affected in the continual learning\nsetup, evaluate a number of continual learning algorithms, and propose a novel\nregularized adapter generation approach. We find that catastrophic forgetting\naffects generalization ability to a less degree than performance on seen tasks;\nwhile continual learning algorithms can still bring considerable benefit to the\ngeneralization ability.\n",
        "id": 94
    },
    {
        "title": "Contrastive Out-of-Distribution Detection for Pretrained Transformers",
        "summary": "  Pretrained Transformers achieve remarkable performance when training and test\ndata are from the same distribution. However, in real-world scenarios, the\nmodel often faces out-of-distribution (OOD) instances that can cause severe\nsemantic shift problems at inference time. Therefore, in practice, a reliable\nmodel should identify such instances, and then either reject them during\ninference or pass them over to models that handle another distribution. In this\npaper, we develop an unsupervised OOD detection method, in which only the\nin-distribution (ID) data are used in training. We propose to fine-tune the\nTransformers with a contrastive loss, which improves the compactness of\nrepresentations, such that OOD instances can be better differentiated from ID\nones. These OOD instances can then be accurately detected using the Mahalanobis\ndistance in the model's penultimate layer. We experiment with comprehensive\nsettings and achieve near-perfect OOD detection performance, outperforming\nbaselines drastically. We further investigate the rationales behind the\nimprovement, finding that more compact representations through margin-based\ncontrastive learning bring the improvement. We release our code to the\ncommunity for future research.\n",
        "id": 95
    },
    {
        "title": "Stream-level Latency Evaluation for Simultaneous Machine Translation",
        "summary": "  Simultaneous machine translation has recently gained traction thanks to\nsignificant quality improvements and the advent of streaming applications.\nSimultaneous translation systems need to find a trade-off between translation\nquality and response time, and with this purpose multiple latency measures have\nbeen proposed. However, latency evaluations for simultaneous translation are\nestimated at the sentence level, not taking into account the sequential nature\nof a streaming scenario. Indeed, these sentence-level latency measures are not\nwell suited for continuous stream translation resulting in figures that are not\ncoherent with the simultaneous translation policy of the system being assessed.\nThis work proposes a stream-level adaptation of the current latency measures\nbased on a re-segmentation approach applied to the output translation, that is\nsuccessfully evaluated on streaming conditions for a reference IWSLT task.\n",
        "id": 96
    },
    {
        "title": "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich\n  Document Understanding",
        "summary": "  Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUND, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUND dataset. The pre-trained\nLayoutXLM model and the XFUND dataset are publicly available at\nhttps://aka.ms/layoutxlm.\n",
        "id": 97
    },
    {
        "title": "Emotion-Regularized Conditional Variational Autoencoder for Emotional\n  Response Generation",
        "summary": "  This paper presents an emotion-regularized conditional variational\nautoencoder (Emo-CVAE) model for generating emotional conversation responses.\nIn conventional CVAE-based emotional response generation, emotion labels are\nsimply used as additional conditions in prior, posterior and decoder networks.\nConsidering that emotion styles are naturally entangled with semantic contents\nin the language space, the Emo-CVAE model utilizes emotion labels to regularize\nthe CVAE latent space by introducing an extra emotion prediction network. In\nthe training stage, the estimated latent variables are required to predict the\nemotion labels and token sequences of the input responses simultaneously.\nExperimental results show that our Emo-CVAE model can learn a more informative\nand structured latent space than a conventional CVAE model and output responses\nwith better content and emotion performance than baseline CVAE and\nsequence-to-sequence (Seq2Seq) models.\n",
        "id": 98
    },
    {
        "title": "Language in a (Search) Box: Grounding Language Learning in Real-World\n  Human-Machine Interaction",
        "summary": "  We investigate grounded language learning through real-world data, by\nmodelling a teacher-learner dynamics through the natural interactions occurring\nbetween users and search engines; in particular, we explore the emergence of\nsemantic generalization from unsupervised dense representations outside of\nsynthetic environments. A grounding domain, a denotation function and a\ncomposition function are learned from user data only. We show how the resulting\nsemantics for noun phrases exhibits compositional properties while being fully\nlearnable without any explicit labelling. We benchmark our grounded semantics\non compositionality and zero-shot inference tasks, and we show that it provides\nbetter results and better generalizations than SOTA non-grounded models, such\nas word2vec and BERT.\n",
        "id": 99
    }
]